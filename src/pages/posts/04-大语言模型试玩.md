<img src="https://raw.githubusercontent.com/Apecice/imagesRepo/main/img/image-20240613161322561.png" vertical-align="middle"/>  

### ollama run llama3

* 下载ollama软件（软件不大，解压才458M左右 - 24年06月）
* `ollama run llama3` - 会直接去下载模型，需要等待一段时间（受限于网络）

![image-20240613162050165](https://raw.githubusercontent.com/Apecice/imagesRepo/main/img/image-20240613162050165.png)

👇 **通过使用本地运行大模型，体验了一下LLM，只是因为配置不够，比如是CPU，而不是GPU来运行的话，跑速非常卡顿，差不多是15秒的间隔回复「远程cURL调用」，默认端口号：11434 。**

![image-20240612111355136](https://raw.githubusercontent.com/Apecice/imagesRepo/main/img/image-20240612111355136.png)

**👇 下面问答画面会卡很久**

<img src="https://raw.githubusercontent.com/Apecice/imagesRepo/main/ollama%E9%97%AE%E7%AD%94.jpg" width=80%>

**以上👆 使用的是meta的llama3:8b的模型，属于80亿参数，占用内存15G左右（Mac Mini是16G的），已经是这样的响应速度，如果是70b模型，即700亿token，占用内存也是15G左右，但是显卡差不多需要70G左右，目前电脑配置根本不敢用，不敢用哇。**



### 知识库

**...了解的不够深入，等研究得差不多时再补充说明下，瓶颈还是在模型调用上：因为网络调用都是在局域网，其次提示词都是源自上传的文档，角色设定已经进行设置，其他的都是默认设置。**
